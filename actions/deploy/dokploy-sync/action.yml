name: 'Dokploy Sync'
description: 'Sync project configuration to Dokploy and trigger deployment (supports applications and compose stacks)'
author: 'NextNodeSolutions'

inputs:
  config-file:
    description: 'Path to project dokploy.toml'
    required: false
    default: 'dokploy.toml'
  defaults-file:
    description: 'Path to defaults TOML file'
    required: false
    default: ''
  environment:
    description: 'Target environment: development, preview, production'
    required: true
  pr-number:
    description: 'PR number (for preview deployments)'
    required: false
    default: ''
  dokploy-url:
    description: 'Dokploy instance URL'
    required: true
  dokploy-admin-email:
    description: 'Dokploy admin email'
    required: true
  dokploy-admin-password:
    description: 'Dokploy admin password'
    required: true
  tailscale-api-token:
    description: 'Tailscale API token for server IP resolution'
    required: true
  cloudflare-api-token:
    description: 'Cloudflare API token for DNS'
    required: true
  action:
    description: 'Action to perform: deploy, cleanup'
    required: false
    default: 'deploy'
  docker-image:
    description: 'Docker image to deploy (e.g., admin-dokploy:5000/repo:tag). When provided, uses Docker source instead of GitHub.'
    required: false
    default: ''
  hcloud-token:
    description: 'Hetzner Cloud API token for accurate server public IP resolution'
    required: false
    default: ''
  compose-file:
    description: 'Path to docker-compose.yml file (for compose deployments)'
    required: false
    default: ''

outputs:
  project-id:
    description: 'Dokploy project ID'
    value: ${{ steps.sync.outputs.project-id }}
  application-id:
    description: 'Dokploy application ID (for application deployments)'
    value: ${{ steps.sync.outputs.application-id }}
  compose-id:
    description: 'Dokploy compose ID (for compose deployments)'
    value: ${{ steps.sync.outputs.compose-id }}
  application-name:
    description: 'Dokploy application/compose name'
    value: ${{ steps.sync.outputs.application-name }}
  domain:
    description: 'Deployed domain'
    value: ${{ steps.sync.outputs.domain }}
  success:
    description: 'Whether the operation succeeded'
    value: ${{ steps.sync.outputs.success }}
  server-ip:
    description: 'Server public IP address'
    value: ${{ steps.sync.outputs.server-ip }}
  cross-swarm:
    description: 'Whether cross-Swarm routing is needed (service on different server than Traefik)'
    value: ${{ steps.sync.outputs.cross-swarm }}
  container-port:
    description: 'Container port the application listens on'
    value: ${{ steps.sync.outputs.container-port }}
  server-name:
    description: 'Target server name where application is deployed'
    value: ${{ steps.sync.outputs.server-name }}
  worker-tailscale-ip:
    description: 'Tailscale IP of the worker server'
    value: ${{ steps.sync.outputs.worker-tailscale-ip }}
  external-port:
    description: 'Deterministic external port for this service (only set when cross-swarm routing is active)'
    value: ${{ steps.port.outputs.external-port }}
  deployment-type:
    description: 'Type of deployment: application or compose'
    value: ${{ steps.sync.outputs.deployment-type }}

runs:
  using: 'composite'
  steps:
    - name: Setup Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.11'

    - name: Install dependencies
      shell: bash
      run: pip install tomli requests

    - name: Sync to Dokploy
      id: sync
      shell: python
      env:
        CONFIG_FILE: ${{ inputs.config-file }}
        DEFAULTS_FILE: ${{ inputs.defaults-file }}
        ENVIRONMENT: ${{ inputs.environment }}
        PR_NUMBER: ${{ inputs.pr-number }}
        DOKPLOY_URL: ${{ inputs.dokploy-url }}
        DOKPLOY_ADMIN_EMAIL: ${{ inputs.dokploy-admin-email }}
        DOKPLOY_ADMIN_PASSWORD: ${{ inputs.dokploy-admin-password }}
        TAILSCALE_API_KEY: ${{ inputs.tailscale-api-token }}
        CLOUDFLARE_API_TOKEN: ${{ inputs.cloudflare-api-token }}
        ACTION: ${{ inputs.action }}
        DOCKER_IMAGE: ${{ inputs.docker-image }}
        HCLOUD_TOKEN: ${{ inputs.hcloud-token }}
        COMPOSE_FILE_INPUT: ${{ inputs.compose-file }}
        GITHUB_OUTPUT: ${{ github.output }}
        GITHUB_REPOSITORY: ${{ github.repository }}
      run: |
        import os
        import sys
        import json
        import re
        import tomli
        import requests
        from pathlib import Path

        # =================================================================
        # CONFIGURATION
        # =================================================================
        CONFIG_FILE = os.environ.get('CONFIG_FILE', 'dokploy.toml')
        DEFAULTS_FILE = os.environ.get('DEFAULTS_FILE', '')
        ENVIRONMENT = os.environ.get('ENVIRONMENT', 'development')
        PR_NUMBER = os.environ.get('PR_NUMBER', '')
        DOKPLOY_URL = os.environ['DOKPLOY_URL'].rstrip('/')
        DOKPLOY_ADMIN_EMAIL = os.environ['DOKPLOY_ADMIN_EMAIL']
        DOKPLOY_ADMIN_PASSWORD = os.environ['DOKPLOY_ADMIN_PASSWORD']
        TAILSCALE_API_KEY = os.environ['TAILSCALE_API_KEY']
        CLOUDFLARE_TOKEN = os.environ['CLOUDFLARE_API_TOKEN']
        ACTION = os.environ.get('ACTION', 'deploy')
        DOCKER_IMAGE = os.environ.get('DOCKER_IMAGE', '')
        HCLOUD_TOKEN = os.environ.get('HCLOUD_TOKEN', '')
        COMPOSE_FILE_INPUT = os.environ.get('COMPOSE_FILE_INPUT', '')
        GITHUB_OUTPUT = os.environ.get('GITHUB_OUTPUT', '')
        GITHUB_REPOSITORY = os.environ.get('GITHUB_REPOSITORY', '')

        def output(key, value):
            if GITHUB_OUTPUT:
                with open(GITHUB_OUTPUT, 'a') as f:
                    f.write(f"{key}={value}\n")

        def deep_merge(base, override):
            """Deep merge two dicts, override wins."""
            result = base.copy()
            for key, value in override.items():
                if key in result and isinstance(result[key], dict) and isinstance(value, dict):
                    result[key] = deep_merge(result[key], value)
                else:
                    result[key] = value
            return result

        def read_env_file(env_path='.env'):
            """Read .env file and return dict of key=value pairs."""
            env_vars = {}
            env_file = Path(env_path)
            if not env_file.exists():
                return env_vars

            for line in env_file.read_text().splitlines():
                line = line.strip()
                if not line or line.startswith('#'):
                    continue
                if '=' in line:
                    key, _, value = line.partition('=')
                    env_vars[key.strip()] = value.strip().strip('"\'')
            return env_vars

        def detect_port():
            """
            Detect application port from multiple sources.
            Priority: .env > Dockerfile ARG > default
            Standard: APP_PORT (NextNode convention)
            """
            # 1. Check .env file for APP_PORT
            env_vars = read_env_file('.env')
            if 'APP_PORT' in env_vars:
                return int(env_vars['APP_PORT']), '.env'

            # 2. Check Dockerfile for ARG APP_PORT=X
            dockerfile = Path('Dockerfile')
            if dockerfile.exists():
                content = dockerfile.read_text()
                match = re.search(r'^ARG\s+APP_PORT\s*=\s*(\d+)', content, re.MULTILINE)
                if match:
                    return int(match.group(1)), 'Dockerfile'

            return None, None

        # =================================================================
        # LOAD AND MERGE CONFIGURATION
        # =================================================================
        print("::group::Loading configuration")

        # Load defaults
        defaults = {}
        if DEFAULTS_FILE and Path(DEFAULTS_FILE).exists():
            with open(DEFAULTS_FILE, 'rb') as f:
                defaults = tomli.load(f)
            print(f"Loaded defaults from: {DEFAULTS_FILE}")

        # Load project config (OPTIONAL now)
        project_config = {}
        if Path(CONFIG_FILE).exists():
            with open(CONFIG_FILE, 'rb') as f:
                project_config = tomli.load(f)
            print(f"Loaded project config from: {CONFIG_FILE}")
        else:
            print(f"No {CONFIG_FILE} found, using defaults only")

        # Merge configs
        config = deep_merge(defaults, project_config)

        # Get project name - infer from GitHub repository if not in config
        project_name = config.get('project', {}).get('name', '')
        if not project_name and GITHUB_REPOSITORY:
            # Extract repo name from "owner/repo" format
            project_name = GITHUB_REPOSITORY.split('/')[-1]
            print(f"Inferred project name from repository: {project_name}")
            # Store inferred name in config for later use
            if 'project' not in config:
                config['project'] = {}
            config['project']['name'] = project_name

        if not project_name:
            print("::error::Could not determine project name (set project.name in dokploy.toml or ensure GITHUB_REPOSITORY is set)")
            output('success', 'false')
            sys.exit(1)

        print(f"Project name: {project_name}")
        print("::endgroup::")

        # =================================================================
        # DETECT COMPOSE MODE
        # =================================================================
        print("::group::Detecting deployment type")

        compose_config = config.get('compose', {})
        is_compose_mode = compose_config.get('enabled', False)

        # Determine compose file path
        compose_file_path = COMPOSE_FILE_INPUT or compose_config.get('file', 'docker-compose.yml')
        compose_file_content = None

        if is_compose_mode:
            print(f"Compose mode enabled")
            compose_path = Path(compose_file_path)
            if compose_path.exists():
                compose_file_content = compose_path.read_text()
                print(f"Loaded compose file: {compose_file_path} ({len(compose_file_content)} bytes)")
            else:
                print(f"::error::Compose file not found: {compose_file_path}")
                output('success', 'false')
                sys.exit(1)
            output('deployment-type', 'compose')
        else:
            print("Application mode (standard deployment)")
            output('deployment-type', 'application')

        print("::endgroup::")

        # =================================================================
        # GENERATE DOMAIN (OPTIONAL)
        # =================================================================
        print("::group::Generating domain")

        base_domain = config.get('project', {}).get('domain', '')

        if base_domain:
            if ENVIRONMENT == 'production':
                domain = base_domain
            elif ENVIRONMENT == 'development':
                domain = f"dev.{base_domain}"
            elif ENVIRONMENT == 'preview' and PR_NUMBER:
                domain = f"pr-{PR_NUMBER}.dev.{base_domain}"
            else:
                domain = f"{ENVIRONMENT}.{base_domain}"
            print(f"Target domain: {domain}")
        else:
            domain = ''
            print("No domain configured, DNS setup will be skipped")

        print("::endgroup::")

        # Detect application port
        print("::group::Port Detection")

        # Config takes priority (explicit override)
        config_port = config.get('project', {}).get('port')
        if config_port:
            app_port = int(config_port)
            print(f"Port: {app_port} (from dokploy.toml)")
        else:
            # Auto-detect from .env or Dockerfile
            detected_port, source = detect_port()
            if detected_port:
                app_port = detected_port
                print(f"Port: {app_port} (auto-detected from {source})")
            else:
                app_port = 3000
                print(f"Port: {app_port} (default)")

        print("::endgroup::")

        # =================================================================
        # SETUP API SESSION (Better Auth)
        # =================================================================
        print("::group::Setting up Dokploy API session")

        # Use session with Bearer token authentication
        session = requests.Session()
        session.headers.update({"Content-Type": "application/json"})

        # Authenticate via Better Auth endpoint
        try:
            auth_resp = session.post(
                f"{DOKPLOY_URL}/api/auth/sign-in/email",
                json={"email": DOKPLOY_ADMIN_EMAIL, "password": DOKPLOY_ADMIN_PASSWORD},
                timeout=30
            )

            if auth_resp.status_code == 401:
                print("::error::Invalid email or password")
                output('success', 'false')
                sys.exit(1)
            auth_resp.raise_for_status()

            # Extract token from response and set as Bearer auth
            auth_data = auth_resp.json()
            token = auth_data.get('token')
            if not token:
                print("::error::No token in auth response")
                output('success', 'false')
                sys.exit(1)

            session.headers.update({"Authorization": f"Bearer {token}"})
            print("Authentication successful")
        except requests.exceptions.RequestException as e:
            print(f"::error::Failed to authenticate with Dokploy: {e}")
            output('success', 'false')
            sys.exit(1)

        # Verify session works by fetching projects
        try:
            test_resp = session.get(f"{DOKPLOY_URL}/api/project.all", timeout=30)
            test_resp.raise_for_status()
            print("API session verified")
        except requests.exceptions.RequestException as e:
            print(f"::error::Session verification failed: {e}")
            output('success', 'false')
            sys.exit(1)

        print("::endgroup::")

        # =================================================================
        # CLEANUP (for PR close)
        # =================================================================
        if ACTION == 'cleanup':
            print("::group::Cleanup preview deployment")
            # Find and delete the preview application or compose
            try:
                projects_resp = session.get(f"{DOKPLOY_URL}/api/project.all", timeout=30)
                projects = projects_resp.json() if projects_resp.ok else []

                project_name = config.get('project', {}).get('name', '')
                preview_name = f"{project_name}-pr-{PR_NUMBER}"

                for project in projects:
                    if project.get('name') == project_name:
                        # Get project details via project.one
                        proj_resp = session.get(
                            f"{DOKPLOY_URL}/api/project.one",
                            params={"projectId": project.get('projectId')},
                            timeout=30
                        )
                        if proj_resp.ok:
                            proj_data = proj_resp.json()

                            # Check for compose stacks first (if compose mode)
                            if is_compose_mode:
                                composes = proj_data.get('compose', [])
                                for env in proj_data.get('environments', []):
                                    composes.extend(env.get('compose', []))
                                for compose in composes:
                                    if compose.get('name') == preview_name:
                                        session.post(
                                            f"{DOKPLOY_URL}/api/compose.delete",
                                            json={"composeId": compose.get('composeId')},
                                            timeout=30
                                        )
                                        print(f"Deleted preview compose: {preview_name}")
                                        break
                            else:
                                # Check for applications
                                apps = proj_data.get('applications', [])
                                for env in proj_data.get('environments', []):
                                    apps.extend(env.get('applications', []))
                                for app in apps:
                                    if app.get('name') == preview_name:
                                        session.post(
                                            f"{DOKPLOY_URL}/api/application.delete",
                                            json={"applicationId": app.get('applicationId')},
                                            timeout=30
                                        )
                                        print(f"Deleted preview application: {preview_name}")
                                        break
            except Exception as e:
                print(f"::warning::Cleanup failed: {e}")

            output('success', 'true')
            print("::endgroup::")
            sys.exit(0)

        # =================================================================
        # GET OR CREATE PROJECT
        # =================================================================
        print("::group::Setting up Dokploy project")

        # project_name already validated and set earlier
        try:
            projects_resp = session.get(f"{DOKPLOY_URL}/api/project.all", timeout=30)
            projects = projects_resp.json() if projects_resp.ok else []

            project_id = None
            for p in projects:
                if p.get('name') == project_name:
                    project_id = p.get('projectId')
                    print(f"Found existing project: {project_name} ({project_id})")
                    break

            if not project_id:
                create_resp = session.post(
                    f"{DOKPLOY_URL}/api/project.create",
                    json={"name": project_name, "description": config.get('project', {}).get('description', '')},
                    timeout=30
                )
                create_resp.raise_for_status()
                create_data = create_resp.json()
                # Handle both possible response formats (projectId or id)
                project_id = create_data.get('projectId') or create_data.get('id')
                if not project_id:
                    print(f"::warning::Unexpected project.create response: {create_data}")
                    # Try to find project by name as fallback
                    projects_resp2 = session.get(f"{DOKPLOY_URL}/api/project.all", timeout=30)
                    for p in projects_resp2.json() if projects_resp2.ok else []:
                        if p.get('name') == project_name:
                            project_id = p.get('projectId')
                            break
                print(f"Created project: {project_name} ({project_id})")
        except Exception as e:
            print(f"::error::Failed to setup project: {e}")
            output('success', 'false')
            sys.exit(1)

        print("::endgroup::")

        # =================================================================
        # GET OR CREATE ENVIRONMENT
        # =================================================================
        print("::group::Setting up Dokploy environment")

        env_name = ENVIRONMENT if ENVIRONMENT != 'preview' else f"preview-{PR_NUMBER}" if PR_NUMBER else 'preview'

        try:
            # Get project details to find environments
            project_resp = session.get(f"{DOKPLOY_URL}/api/project.one?projectId={project_id}", timeout=30)
            project_data = project_resp.json() if project_resp.ok else {}
            environments = project_data.get('environments', [])

            environment_id = None
            for env in environments:
                if env.get('name') == env_name:
                    environment_id = env.get('environmentId')
                    print(f"Found existing environment: {env_name} ({environment_id})")
                    break

            if not environment_id:
                # Create environment
                create_env_resp = session.post(
                    f"{DOKPLOY_URL}/api/environment.create",
                    json={"projectId": project_id, "name": env_name, "description": f"Environment for {env_name}"},
                    timeout=30
                )
                if create_env_resp.ok:
                    environment_id = create_env_resp.json().get('environmentId')
                    print(f"Created environment: {env_name} ({environment_id})")
                else:
                    print(f"Failed to create environment: {create_env_resp.text}")
                    # Try to get the default environment
                    if environments:
                        environment_id = environments[0].get('environmentId')
                        print(f"Using default environment: {environment_id}")

            if not environment_id:
                print("::error::Could not get or create environment")
                output('success', 'false')
                sys.exit(1)

        except Exception as e:
            print(f"::error::Failed to setup environment: {e}")
            output('success', 'false')
            sys.exit(1)

        print("::endgroup::")

        # =================================================================
        # GET SERVER INFO
        # =================================================================
        print("::group::Resolving server")

        env_config = config.get('environments', {}).get(ENVIRONMENT, {})
        if ENVIRONMENT == 'preview':
            env_config = config.get('environments', {}).get('preview', config.get('environments', {}).get('development', {}))

        server_name = env_config.get('server', 'dev-worker' if ENVIRONMENT != 'production' else 'prod-worker')

        # Override from environment-specific config in project
        if ENVIRONMENT == 'production' and 'production' in config:
            server_name = config['production'].get('server', server_name)

        print(f"Target server: {server_name}")

        # Get server ID from Dokploy and public IP from Hetzner
        server_public_ip = None
        server_id = None

        # Step 1: Get public IP from Hetzner Cloud API (accurate source)
        if HCLOUD_TOKEN:
            try:
                hetzner_resp = requests.get(
                    f"https://api.hetzner.cloud/v1/servers?name={server_name}",
                    headers={"Authorization": f"Bearer {HCLOUD_TOKEN}"},
                    timeout=30
                )
                if hetzner_resp.ok:
                    servers_data = hetzner_resp.json().get('servers', [])
                    if servers_data:
                        ipv4_data = servers_data[0].get('public_net', {}).get('ipv4', {})
                        server_public_ip = ipv4_data.get('ip')
                        if server_public_ip:
                            print(f"Server public IP (from Hetzner): {server_public_ip}")
                    else:
                        print(f"::warning::Server {server_name} not found in Hetzner Cloud")
                else:
                    print(f"::warning::Hetzner API returned {hetzner_resp.status_code}")
            except Exception as e:
                print(f"::warning::Failed to get IP from Hetzner: {e}")

        # Step 2: Get server ID from Dokploy (needed for deployment)
        try:
            servers_resp = session.get(f"{DOKPLOY_URL}/api/server.all", timeout=30)
            servers = servers_resp.json() if servers_resp.ok else []

            for s in servers:
                if s.get('name') == server_name:
                    server_id = s.get('serverId')
                    print(f"Found server in Dokploy: {server_name} ({server_id})")

                    # Fallback: Get IP from Dokploy if Hetzner failed
                    if not server_public_ip:
                        try:
                            ip_resp = session.get(
                                f"{DOKPLOY_URL}/api/server.publicIp",
                                params={"serverId": server_id},
                                timeout=30
                            )
                            if ip_resp.ok:
                                ip_data = ip_resp.json()
                                if isinstance(ip_data, str):
                                    server_public_ip = ip_data if ip_data else None
                                elif isinstance(ip_data, dict):
                                    server_public_ip = ip_data.get('public_ip') or ip_data.get('publicIp') or ip_data.get('ip')
                                if server_public_ip:
                                    print(f"Server public IP (from Dokploy fallback): {server_public_ip}")
                        except Exception as e:
                            print(f"::warning::Failed to get server IP from Dokploy: {e}")
                    break

            if not server_id:
                print(f"::warning::Server {server_name} not found in Dokploy, using default")
        except Exception as e:
            print(f"::warning::Failed to get servers from Dokploy: {e}")
            server_id = None

        print("::endgroup::")

        # =================================================================
        # GET TRAEFIK SERVER IP (for DNS - all traffic routes through Traefik)
        # =================================================================
        print("::group::Getting Traefik server IP")

        # In Swarm mode, all DNS records point to the Traefik ingress server
        traefik_server = config.get('cluster', {}).get('traefik-server', 'admin-dokploy')
        traefik_public_ip = None

        print(f"Traefik server (from cluster config): {traefik_server}")

        if HCLOUD_TOKEN:
            try:
                traefik_resp = requests.get(
                    f"https://api.hetzner.cloud/v1/servers?name={traefik_server}",
                    headers={"Authorization": f"Bearer {HCLOUD_TOKEN}"},
                    timeout=30
                )
                if traefik_resp.ok:
                    traefik_servers = traefik_resp.json().get('servers', [])
                    if traefik_servers:
                        traefik_ipv4 = traefik_servers[0].get('public_net', {}).get('ipv4', {})
                        traefik_public_ip = traefik_ipv4.get('ip')
                        if traefik_public_ip:
                            print(f"Traefik server public IP: {traefik_public_ip}")
                    else:
                        print(f"::warning::Traefik server {traefik_server} not found in Hetzner Cloud")
                else:
                    print(f"::warning::Hetzner API returned {traefik_resp.status_code}")
            except Exception as e:
                print(f"::warning::Failed to get Traefik server IP from Hetzner: {e}")

        # Fallback to worker server IP if Traefik lookup fails (backwards compatibility)
        if not traefik_public_ip:
            traefik_public_ip = server_public_ip
            if traefik_public_ip:
                print(f"::warning::Using worker server IP as fallback: {traefik_public_ip}")

        print("::endgroup::")

        # =================================================================
        # CREATE/UPDATE APPLICATION OR COMPOSE
        # =================================================================
        app_name = project_name if ENVIRONMENT == 'production' else f"{project_name}-{ENVIRONMENT}"
        if ENVIRONMENT == 'preview' and PR_NUMBER:
            app_name = f"{project_name}-pr-{PR_NUMBER}"

        # Initialize IDs
        app_id = None
        compose_id = None

        if is_compose_mode:
            # =================================================================
            # COMPOSE MODE: Create/Update Compose Stack
            # =================================================================
            print("::group::Setting up compose stack")

            try:
                # Check if compose exists by getting project details
                project_detail_resp = session.get(
                    f"{DOKPLOY_URL}/api/project.one",
                    params={"projectId": project_id},
                    timeout=30
                )

                composes = []
                if project_detail_resp.ok:
                    project_detail = project_detail_resp.json()

                    # Compose stacks are nested under environments in Dokploy API
                    composes = project_detail.get('compose', [])
                    for env in project_detail.get('environments', []):
                        composes.extend(env.get('compose', []))

                    print(f"Found {len(composes)} compose stacks in project {project_name}")
                else:
                    print(f"::warning::Failed to get project details: {project_detail_resp.status_code}")

                # Find existing compose by name
                for c in composes:
                    if c.get('name') == app_name:
                        compose_id = c.get('composeId')
                        print(f"Found existing compose: {app_name} ({compose_id})")
                        break

                if not compose_id:
                    # Create compose stack
                    compose_data = {
                        "name": app_name,
                        "projectId": project_id,
                        "environmentId": environment_id,
                        "composeType": "docker-compose",
                    }
                    if server_id:
                        compose_data["serverId"] = server_id

                    create_compose_resp = session.post(
                        f"{DOKPLOY_URL}/api/compose.create",
                        json=compose_data,
                        timeout=30
                    )
                    if not create_compose_resp.ok:
                        print(f"::warning::compose.create response: {create_compose_resp.status_code} - {create_compose_resp.text[:500]}")
                    create_compose_resp.raise_for_status()
                    compose_id = create_compose_resp.json().get('composeId')
                    print(f"Created compose stack: {app_name} ({compose_id})")

                # Update compose with file content (sourceType: raw)
                update_data = {
                    "composeId": compose_id,
                    "sourceType": "raw",
                    "composeFile": compose_file_content,
                }

                update_resp = session.post(
                    f"{DOKPLOY_URL}/api/compose.update",
                    json=update_data,
                    timeout=30
                )
                if update_resp.ok:
                    print(f"Compose file updated ({len(compose_file_content)} bytes)")
                else:
                    print(f"::warning::Compose update response: {update_resp.status_code} - {update_resp.text[:200]}")

            except Exception as e:
                print(f"::error::Failed to setup compose: {e}")
                output('success', 'false')
                sys.exit(1)

            print("::endgroup::")

        else:
            # =================================================================
            # APPLICATION MODE: Create/Update Application
            # =================================================================
            print("::group::Setting up application")

            # Get build config
            build_config = config.get('build', {})
            build_type = build_config.get('type', 'dockerfile')

            # Get resource config (merge defaults with environment-specific)
            resources = config.get('resources', {}).copy()
            if ENVIRONMENT == 'production' and 'production' in config:
                resources.update({k: v for k, v in config['production'].items() if k in ['memory', 'cpu', 'replicas']})

            try:
                # Check if app exists by getting project details which include applications
                # The application.all endpoint doesn't exist - apps are nested in project.one response
                project_detail_resp = session.get(
                    f"{DOKPLOY_URL}/api/project.one",
                    params={"projectId": project_id},
                    timeout=30
                )

                apps = []
                if project_detail_resp.ok:
                    project_detail = project_detail_resp.json()

                    # Applications are nested under environments in Dokploy API
                    for env in project_detail.get('environments', []):
                        apps.extend(env.get('applications', []))

                    print(f"Found {len(apps)} applications in project {project_name}")
                else:
                    print(f"::warning::Failed to get project details: {project_detail_resp.status_code}")

                app_exists = False
                for a in apps:
                    if a.get('name') == app_name:
                        app_id = a.get('applicationId')
                        app_exists = True
                        print(f"Found existing application: {app_name} ({app_id})")
                        break

                # Get GitHub repository URL
                github_repo = os.environ.get('GITHUB_REPOSITORY', '')
                github_url = f"https://github.com/{github_repo}" if github_repo else ''

                if not app_id:
                    # Create application
                    app_data = {
                        "name": app_name,
                        "projectId": project_id,
                        "environmentId": environment_id,
                        "buildType": build_type,
                    }
                    if server_id:
                        app_data["serverId"] = server_id

                    create_app_resp = session.post(
                        f"{DOKPLOY_URL}/api/application.create",
                        json=app_data,
                        timeout=30
                    )
                    create_app_resp.raise_for_status()
                    app_id = create_app_resp.json().get('applicationId')
                    print(f"Created application: {app_name} ({app_id})")

                # Configure source: Docker image takes precedence, else GitHub
                if DOCKER_IMAGE:
                    print(f"Configuring Docker source: {DOCKER_IMAGE}")
                    source_data = {
                        "applicationId": app_id,
                        "sourceType": "docker",
                        "dockerImage": DOCKER_IMAGE,
                    }
                    try:
                        source_resp = session.post(
                            f"{DOKPLOY_URL}/api/application.update",
                            json=source_data,
                            timeout=30
                        )
                        if source_resp.ok:
                            print(f"Application configured with Docker image: {DOCKER_IMAGE}")
                        else:
                            print(f"::warning::Docker source config response: {source_resp.status_code} - {source_resp.text[:200]}")
                    except Exception as e:
                        print(f"::warning::Failed to configure Docker source: {e}")
                elif github_url:
                    # Fallback to GitHub source if no Docker image provided
                    source_data = {
                        "applicationId": app_id,
                        "sourceType": "github",
                        "customGitUrl": github_url,
                        "customGitBranch": "main"
                    }
                    try:
                        source_resp = session.post(
                            f"{DOKPLOY_URL}/api/application.update",
                            json=source_data,
                            timeout=30
                        )
                        if source_resp.ok:
                            print(f"Source configured: {github_url}")
                        else:
                            print(f"::warning::Source config response: {source_resp.status_code} - {source_resp.text[:200]}")
                    except Exception as e:
                        print(f"::warning::Failed to configure source: {e}")

            except Exception as e:
                print(f"::error::Failed to setup application: {e}")
                output('success', 'false')
                sys.exit(1)

            print("::endgroup::")

        # =================================================================
        # CONFIGURE DOMAIN (only if domain is set and not compose mode)
        # Note: Compose stacks manage their own domains via docker-compose labels
        # =================================================================
        if not is_compose_mode:
            print("::group::Configuring domain")

            if domain:
                try:
                    domain_data = {
                        "applicationId": app_id,
                        "host": domain,
                        "port": app_port,
                        "https": True,
                        "certificateType": "letsencrypt"
                    }

                    session.post(
                        f"{DOKPLOY_URL}/api/domain.create",
                        json=domain_data,
                        timeout=30
                    )
                    print(f"Domain configured: {domain} (port {app_port})")
                except Exception as e:
                    print(f"::warning::Domain configuration: {e}")
            else:
                print("No domain configured, skipping domain setup")

            print("::endgroup::")
        else:
            print("::group::Domain configuration")
            print("Compose mode: domains are managed via Traefik labels in docker-compose.yml")
            print("::endgroup::")

        # =================================================================
        # CONFIGURE HEALTH CHECK (Docker Swarm format)
        # =================================================================
        if not is_compose_mode and app_id:
            print("::group::Configuring health check")

            healthcheck_config = config.get('healthcheck', {})
            hc_enabled = healthcheck_config.get('enabled', True)

            if hc_enabled:
                hc_path = healthcheck_config.get('path', '/health')
                # Use config port if explicitly set, otherwise use detected app_port
                hc_port = healthcheck_config.get('port', app_port)
                # Convert seconds to nanoseconds for Docker Swarm format
                hc_interval_ns = healthcheck_config.get('interval', 30) * 1_000_000_000
                hc_timeout_ns = healthcheck_config.get('timeout', 10) * 1_000_000_000
                hc_retries = healthcheck_config.get('retries', 3)
                hc_start_period_ns = healthcheck_config.get('start_period', 40) * 1_000_000_000

                # Docker Swarm healthcheck format
                healthcheck_swarm = {
                    "Test": ["CMD", "curl", "-f", f"http://localhost:{hc_port}{hc_path}"],
                    "Interval": hc_interval_ns,
                    "Timeout": hc_timeout_ns,
                    "StartPeriod": hc_start_period_ns,
                    "Retries": hc_retries
                }

                # Rollback config (rollback to previous version if healthcheck fails)
                rollback_enabled = healthcheck_config.get('rollback', True)
                update_delay_ns = healthcheck_config.get('update_delay', 10) * 1_000_000_000

                update_config_swarm = {
                    "Parallelism": 1,
                    "Delay": update_delay_ns,
                    "FailureAction": "rollback" if rollback_enabled else "pause",
                    "Order": "start-first"  # Zero-downtime: start new before stopping old
                }

                try:
                    hc_data = {
                        "applicationId": app_id,
                        "healthCheckSwarm": healthcheck_swarm,
                        "updateConfigSwarm": update_config_swarm,
                    }
                    hc_resp = session.post(f"{DOKPLOY_URL}/api/application.update", json=hc_data, timeout=30)
                    if hc_resp.ok:
                        print(f"Health check configured: curl -f http://localhost:{hc_port}{hc_path}")
                        print(f"Update config: start-first, {'auto-rollback' if rollback_enabled else 'pause'} on failure")
                    else:
                        print(f"::warning::Health check config response: {hc_resp.status_code} - {hc_resp.text[:200]}")
                except Exception as e:
                    print(f"::warning::Health check configuration failed: {e}")
            else:
                print("Health check disabled in config")

            print("::endgroup::")

        # =================================================================
        # CROSS-SWARM ROUTING DETECTION
        # =================================================================
        print("::group::Cross-Swarm Detection")

        traefik_server = config.get('cluster', {}).get('traefik-server', 'admin-dokploy')
        needs_cross_swarm = server_name != traefik_server and domain

        if needs_cross_swarm:
            print(f"Cross-Swarm routing needed: {server_name} != {traefik_server}")

            # Get worker's Tailscale IP for cross-Swarm routing
            worker_tailscale_ip = None
            try:
                # Query Tailscale API for device by hostname
                ts_resp = requests.get(
                    "https://api.tailscale.com/api/v2/tailnet/-/devices",
                    headers={"Authorization": f"Bearer {TAILSCALE_API_KEY}"},
                    timeout=30
                )
                if ts_resp.ok:
                    devices = ts_resp.json().get('devices', [])
                    for device in devices:
                        # Match by hostname (server_name is the hostname)
                        if device.get('hostname', '').lower() == server_name.lower():
                            addresses = device.get('addresses', [])
                            # Get first IPv4 address (Tailscale IPs are typically 100.x.x.x)
                            for addr in addresses:
                                if addr.startswith('100.'):
                                    worker_tailscale_ip = addr
                                    print(f"Worker Tailscale IP: {worker_tailscale_ip}")
                                    break
                            break
                    if not worker_tailscale_ip:
                        print(f"::warning::Could not find Tailscale IP for {server_name}")
                else:
                    print(f"::warning::Tailscale API returned {ts_resp.status_code}")
            except Exception as e:
                print(f"::warning::Failed to get Tailscale IP: {e}")

            output('cross-swarm', 'true')
            output('worker-tailscale-ip', worker_tailscale_ip or '')
        else:
            print(f"Same-Swarm deployment ({server_name} == {traefik_server}), no cross-Swarm routing needed")
            output('cross-swarm', 'false')
            output('worker-tailscale-ip', '')

        output('container-port', str(app_port))
        output('server-name', server_name)

        print("::endgroup::")

        # =================================================================
        # TRIGGER DEPLOYMENT
        # =================================================================
        print("::group::Triggering deployment")

        try:
            if is_compose_mode:
                deploy_resp = session.post(
                    f"{DOKPLOY_URL}/api/compose.deploy",
                    json={"composeId": compose_id},
                    timeout=60
                )
                if deploy_resp.ok:
                    print(f"Compose deployment triggered for {app_name}")
                else:
                    print(f"::warning::Compose deployment trigger response: {deploy_resp.status_code}")
            else:
                deploy_resp = session.post(
                    f"{DOKPLOY_URL}/api/application.deploy",
                    json={"applicationId": app_id},
                    timeout=60
                )
                if deploy_resp.ok:
                    print(f"Application deployment triggered for {app_name}")
                else:
                    print(f"::warning::Application deployment trigger response: {deploy_resp.status_code}")
        except Exception as e:
            print(f"::warning::Failed to trigger deployment: {e}")

        print("::endgroup::")

        # =================================================================
        # OUTPUT RESULTS
        # =================================================================
        output('project-id', project_id)
        output('application-id', app_id or '')
        output('compose-id', compose_id or '')
        output('application-name', app_name)
        output('domain', domain)
        # Use Traefik server IP for DNS (all traffic routes through Traefik in Swarm mode)
        output('server-ip', traefik_public_ip or '')
        output('success', 'true')

        print("")
        print("=" * 60)
        print(f"Project: {project_name}")
        if is_compose_mode:
            print(f"Compose: {app_name}")
        else:
            print(f"Application: {app_name}")
        if domain:
            print(f"Domain: https://{domain}")
        else:
            print("Domain: (not configured)")
        print(f"Environment: {ENVIRONMENT}")
        print("=" * 60)

    - name: Publish service port (optional)
      id: port
      if: ${{ steps.sync.outputs.worker-tailscale-ip != '' && steps.sync.outputs.success == 'true' }}
      uses: nextnodesolutions/github-actions/actions/deploy/publish-service-port@main
      with:
        app-name: ${{ steps.sync.outputs.application-name }}
        container-port: ${{ steps.sync.outputs.container-port }}
        server-ip: ${{ steps.sync.outputs.worker-tailscale-ip }}
        expected-image: ${{ inputs.docker-image }}

name: 'VPS Provision'
description: 'Provision and register a custom VPS for Dokploy deployments. Composes atomic actions: vps-status-check, terraform apply, vps-tailscale-wait, vps-dokploy-register, vps-docker-wait, vps-node-label'
author: 'NextNodeSolutions'

inputs:
  vps-name:
    description: 'Name of the VPS to provision'
    required: true
  project-name:
    description: 'Project name for tagging'
    required: true
  server-type:
    description: 'Hetzner server type (e.g., cx33)'
    required: false
    default: 'cx33'
  location:
    description: 'Hetzner datacenter location'
    required: false
    default: 'nbg1'
  environment:
    description: 'Environment name'
    required: false
    default: 'production'
  hetzner-token:
    description: 'Hetzner Cloud API token'
    required: true
  tailscale-auth-key:
    description: 'Tailscale auth key for VPS to join network'
    required: true
  tailscale-api-token:
    description: 'Tailscale API token for status checks'
    required: true
  cloudflare-api-token:
    description: 'Cloudflare API token (for Terraform)'
    required: true
  tf-api-token:
    description: 'Terraform Cloud API token'
    required: true
  github-token:
    description: 'GitHub token with access to infrastructure repo'
    required: true
  dokploy-url:
    description: 'Dokploy instance URL'
    required: true
  dokploy-api-token:
    description: 'Dokploy API token (from Settings > Profile > API/CLI)'
    required: true
  orchestration-secret:
    description: 'Orchestration secret for worker registration'
    required: false
    default: ''
  manager-host:
    description: 'Hostname of the Swarm manager'
    required: false
    default: 'admin-dokploy'
  has-volume:
    description: 'Whether to create a persistent volume'
    required: false
    default: 'true'
  volume-size:
    description: 'Size of persistent volume in GB'
    required: false
    default: '10'
  force-replace:
    description: 'Force replace VPS using terraform apply -replace (atomic destroy+create)'
    required: false
    default: 'false'
  force-destroy-recreate:
    description: 'Force full destroy then recreate VPS (non-atomic, use when -replace fails)'
    required: false
    default: 'false'

outputs:
  provisioned:
    description: 'Whether a new VPS was provisioned'
    value: ${{ steps.set-outputs.outputs.provisioned }}
  server-ip:
    description: 'Public IP of the VPS'
    value: ${{ steps.set-outputs.outputs.server-ip }}
  tailscale-ip:
    description: 'Tailscale IP of the VPS'
    value: ${{ steps.set-outputs.outputs.tailscale-ip }}
  server-id:
    description: 'Dokploy server ID'
    value: ${{ steps.register.outputs.server-id }}
  docker-ready:
    description: 'Whether Docker is ready on the VPS'
    value: ${{ steps.docker-wait.outputs.success }}
  node-labeled:
    description: 'Whether Swarm node label was applied'
    value: ${{ steps.node-label.outputs.success }}
  success:
    description: 'Whether provisioning succeeded (includes Docker ready and node labeled for new VPS)'
    value: ${{ steps.final-status.outputs.success }}

runs:
  using: 'composite'
  steps:
    # Step 0: Compute config hash for change detection
    # Only include immutable infrastructure properties - environment changes should NOT trigger VPS recreation
    - name: Compute Config Hash
      id: config-hash
      shell: bash
      env:
        SERVER_TYPE: ${{ inputs.server-type }}
        LOCATION: ${{ inputs.location }}
        HAS_VOLUME: ${{ inputs.has-volume }}
        VOLUME_SIZE: ${{ inputs.volume-size }}
      run: |
        CONFIG="${SERVER_TYPE}|${LOCATION}|${HAS_VOLUME}|${VOLUME_SIZE}"
        HASH=$(echo -n "$CONFIG" | sha256sum | cut -c1-12)
        echo "hash=$HASH" >> $GITHUB_OUTPUT
        echo "Config: $CONFIG -> Hash: $HASH"

    # Step 1: Check VPS status in Hetzner and Tailscale
    - name: Check VPS Status
      id: check
      uses: nextnodesolutions/github-actions/actions/deploy/vps-status-check@main
      with:
        vps-name: ${{ inputs.vps-name }}
        hcloud-token: ${{ inputs.hetzner-token }}
        tailscale-api-key: ${{ inputs.tailscale-api-token }}
        expected-config-hash: ${{ steps.config-hash.outputs.hash }}
        force-replace: ${{ inputs.force-replace }}
        force-destroy-recreate: ${{ inputs.force-destroy-recreate }}

    # Step 2: Checkout infrastructure repo for Terraform
    - name: Checkout Infrastructure
      if: steps.check.outputs.needs-provision == 'true' || steps.check.outputs.needs-destroy == 'true' || steps.check.outputs.needs-destroy-recreate == 'true'
      uses: actions/checkout@v4
      with:
        repository: nextnodesolutions/infrastructure
        path: .infrastructure
        token: ${{ inputs.github-token }}

    # Step 3: Setup Terraform
    - name: Setup Terraform
      if: steps.check.outputs.needs-provision == 'true' || steps.check.outputs.needs-destroy == 'true' || steps.check.outputs.needs-destroy-recreate == 'true'
      uses: hashicorp/setup-terraform@v3
      with:
        terraform_version: '1.5.7'
        cli_config_credentials_token: ${{ inputs.tf-api-token }}
        terraform_wrapper: false

    # Step 4a: Terraform Destroy (only for force-destroy-recreate)
    - name: Terraform Destroy
      id: destroy
      if: steps.check.outputs.needs-destroy-recreate == 'true'
      shell: bash
      working-directory: .infrastructure/terraform/single-vps
      env:
        TF_VAR_hetzner_token: ${{ inputs.hetzner-token }}
        TF_VAR_vps_name: ${{ inputs.vps-name }}
        TF_VAR_project_name: ${{ inputs.project-name }}
        TF_VAR_server_type: ${{ inputs.server-type }}
        TF_VAR_location: ${{ inputs.location }}
        TF_VAR_environment: ${{ inputs.environment }}
        TF_VAR_tailscale_auth_key: ${{ inputs.tailscale-auth-key }}
        TF_VAR_cloudflare_dns_token: ${{ inputs.cloudflare-api-token }}
        TF_VAR_manager_host: ${{ inputs.manager-host }}
        TF_VAR_orchestration_secret: ${{ inputs.orchestration-secret }}
        TF_VAR_config_hash: ${{ steps.config-hash.outputs.hash }}
        TF_VAR_has_volume: ${{ inputs.has-volume }}
        TF_VAR_volume_size: ${{ inputs.volume-size }}
      run: |
        echo "::group::Terraform Destroy (force-destroy-recreate)"
        echo "Destroying VPS: $TF_VAR_vps_name"
        echo "This is a non-atomic operation - VPS will be fully destroyed before recreation"

        terraform init
        terraform destroy -auto-approve

        echo "VPS destroyed successfully"
        echo "::endgroup::"

    # Step 4b: Provision VPS with Terraform
    # Uses -replace flag for atomic destroy+create when replacing existing VPS
    - name: Terraform Apply
      id: provision
      if: steps.check.outputs.needs-provision == 'true' || steps.check.outputs.needs-destroy == 'true' || steps.check.outputs.needs-destroy-recreate == 'true'
      shell: bash
      working-directory: .infrastructure/terraform/single-vps
      env:
        TF_VAR_hetzner_token: ${{ inputs.hetzner-token }}
        TF_VAR_vps_name: ${{ inputs.vps-name }}
        TF_VAR_project_name: ${{ inputs.project-name }}
        TF_VAR_server_type: ${{ inputs.server-type }}
        TF_VAR_location: ${{ inputs.location }}
        TF_VAR_environment: ${{ inputs.environment }}
        TF_VAR_tailscale_auth_key: ${{ inputs.tailscale-auth-key }}
        TF_VAR_cloudflare_dns_token: ${{ inputs.cloudflare-api-token }}
        TF_VAR_manager_host: ${{ inputs.manager-host }}
        TF_VAR_orchestration_secret: ${{ inputs.orchestration-secret }}
        TF_VAR_config_hash: ${{ steps.config-hash.outputs.hash }}
        TF_VAR_has_volume: ${{ inputs.has-volume }}
        TF_VAR_volume_size: ${{ inputs.volume-size }}
        NEEDS_REPLACE: ${{ steps.check.outputs.needs-destroy }}
      run: |
        echo "::group::Terraform Apply"
        echo "VPS: $TF_VAR_vps_name ($TF_VAR_server_type) in $TF_VAR_location"
        echo "Config hash: $TF_VAR_config_hash"
        echo "Volume: ${TF_VAR_has_volume} (${TF_VAR_volume_size}GB)"
        echo "Replace mode: $NEEDS_REPLACE"

        terraform init

        if [[ "$NEEDS_REPLACE" == "true" ]]; then
          # Atomic destroy+create via -replace flag
          # Maintains state consistency and provides rollback if creation fails
          echo "Using terraform apply -replace for atomic VPS replacement"
          terraform apply -auto-approve -replace="module.vps.hcloud_server.this"
        else
          # Normal apply for new VPS (or after destroy-recreate)
          echo "Creating new VPS"
          terraform apply -auto-approve
        fi

        # Get outputs
        SERVER_ID=$(terraform output -raw server_id 2>/dev/null)
        IPV4=$(terraform output -raw ipv4_address 2>/dev/null)

        echo "server-id=$SERVER_ID" >> $GITHUB_OUTPUT
        echo "server-ip=$IPV4" >> $GITHUB_OUTPUT

        echo "VPS provisioned: $TF_VAR_vps_name ($IPV4)"
        echo "::endgroup::"

    # Step 5: Wait for Tailscale if VPS was just provisioned or recreated
    - name: Wait for Tailscale
      id: wait-tailscale
      if: steps.check.outputs.needs-provision == 'true' || steps.check.outputs.needs-destroy == 'true' || steps.check.outputs.needs-destroy-recreate == 'true'
      uses: nextnodesolutions/github-actions/actions/deploy/vps-tailscale-wait@main
      with:
        vps-name: ${{ inputs.vps-name }}
        tailscale-api-key: ${{ inputs.tailscale-api-token }}
        timeout-seconds: '300'

    # Step 6: Consolidate outputs
    - name: Set Outputs
      id: set-outputs
      shell: bash
      env:
        PROVISIONED: ${{ steps.check.outputs.needs-provision }}
        PROVISION_IP: ${{ steps.provision.outputs.server-ip }}
        HETZNER_IP: ${{ steps.check.outputs.hetzner-ip }}
        WAIT_TAILSCALE_IP: ${{ steps.wait-tailscale.outputs.tailscale-ip }}
        EXISTING_TAILSCALE_IP: ${{ steps.check.outputs.tailscale-ip }}
      run: |
        echo "::group::Consolidate Outputs"

        # Determine server IP
        if [[ -n "$PROVISION_IP" ]]; then
          echo "server-ip=$PROVISION_IP" >> $GITHUB_OUTPUT
        else
          echo "server-ip=$HETZNER_IP" >> $GITHUB_OUTPUT
        fi

        # Determine Tailscale IP
        if [[ -n "$WAIT_TAILSCALE_IP" ]]; then
          TAILSCALE_IP="$WAIT_TAILSCALE_IP"
        else
          TAILSCALE_IP="$EXISTING_TAILSCALE_IP"
        fi
        echo "tailscale-ip=$TAILSCALE_IP" >> $GITHUB_OUTPUT

        # Set provisioned flag
        echo "provisioned=$PROVISIONED" >> $GITHUB_OUTPUT

        echo "::endgroup::"

    # Step 7: Register VPS in Dokploy
    # Use Tailscale IP - Dokploy reaches Tailscale network via host routing
    - name: Register in Dokploy
      id: register
      uses: nextnodesolutions/github-actions/actions/deploy/vps-dokploy-register@main
      with:
        dokploy-url: ${{ inputs.dokploy-url }}
        dokploy-api-token: ${{ inputs.dokploy-api-token }}
        server-name: ${{ inputs.vps-name }}
        server-ip: ${{ steps.set-outputs.outputs.tailscale-ip }}

    # Step 8: Wait for Docker to be ready (if newly provisioned)
    - name: Wait for Docker
      id: docker-wait
      if: steps.check.outputs.needs-provision == 'true' || steps.register.outputs.registered == 'true'
      uses: nextnodesolutions/github-actions/actions/deploy/vps-docker-wait@main
      with:
        server-ip: ${{ steps.set-outputs.outputs.tailscale-ip }}
        timeout-seconds: '300'

    # Step 9: Get Traefik server Tailscale IP for node labeling
    - name: Lookup Manager IP
      id: manager-lookup
      if: steps.check.outputs.needs-provision == 'true' || steps.register.outputs.registered == 'true'
      uses: nextnodesolutions/github-actions/actions/utilities/tailscale-device-lookup@main
      with:
        device-name: 'admin-dokploy'
        tailscale-api-key: ${{ inputs.tailscale-api-token }}

    # Step 10: Apply Swarm node label for placement constraints
    - name: Apply Node Label
      id: node-label
      if: (steps.check.outputs.needs-provision == 'true' || steps.register.outputs.registered == 'true') && steps.manager-lookup.outputs.tailscale-ip != ''
      uses: nextnodesolutions/github-actions/actions/deploy/vps-node-label@main
      with:
        manager-ip: ${{ steps.manager-lookup.outputs.tailscale-ip }}
        node-name: ${{ inputs.vps-name }}
        label-key: 'server'
        label-value: ${{ inputs.vps-name }}

    # Step 11: Final status check
    - name: Final Status
      id: final-status
      shell: bash
      env:
        REGISTER_SUCCESS: ${{ steps.register.outputs.success }}
        DOCKER_WAIT_SUCCESS: ${{ steps.docker-wait.outputs.success }}
        NODE_LABEL_SUCCESS: ${{ steps.node-label.outputs.success }}
        NEEDS_PROVISION: ${{ steps.check.outputs.needs-provision }}
        NEWLY_REGISTERED: ${{ steps.register.outputs.registered }}
      run: |
        echo "::group::Final Status Check"

        # Registration must succeed
        if [[ "$REGISTER_SUCCESS" != "true" ]]; then
          echo "::error::VPS registration failed"
          echo "success=false" >> $GITHUB_OUTPUT
          echo "::endgroup::"
          exit 0
        fi

        # For newly provisioned or registered VPS, Docker must be ready
        if [[ "$NEEDS_PROVISION" == "true" || "$NEWLY_REGISTERED" == "true" ]]; then
          if [[ "$DOCKER_WAIT_SUCCESS" != "true" ]]; then
            echo "::error::Docker is not ready on VPS"
            echo "success=false" >> $GITHUB_OUTPUT
            echo "::endgroup::"
            exit 0
          fi

          if [[ "$NODE_LABEL_SUCCESS" != "true" ]]; then
            echo "::warning::Node label not applied (deployment may fail placement constraints)"
            # Don't fail - label can be applied manually if needed
          fi
        fi

        echo "VPS provisioning complete"
        echo "success=true" >> $GITHUB_OUTPUT
        echo "::endgroup::"
